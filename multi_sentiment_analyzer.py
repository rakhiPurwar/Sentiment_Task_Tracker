# -*- coding: utf-8 -*-
"""Multi Sentiment Analyzer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OiB22-ZmyFnl50pyeiMTJIZe6kLt1MpV
"""

!pip install tweet-preprocessor
!pip install emoji
!pip install transformers
!pip install tokenizers

import preprocessor as p
import numpy as np 
import pandas as pd 
import emoji
import keras
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import Sequential
from keras.layers.recurrent import LSTM, GRU,SimpleRNN
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.embeddings import Embedding
from keras.layers.normalization import BatchNormalization
from keras.utils import np_utils
from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline
from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D
from keras.preprocessing import sequence, text
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
import plotly.graph_objects as go
import plotly.express as px
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
import transformers
from transformers import TFAutoModel, AutoTokenizer
from tqdm.notebook import tqdm
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors
from tqdm import tqdm

data = pd.read_csv("text_emotion.csv")

misspell_data = pd.read_csv("aspell.txt",sep=":",names=["correction","misspell"])
misspell_data.misspell = misspell_data.misspell.str.strip()
misspell_data.misspell = misspell_data.misspell.str.split(" ")
misspell_data = misspell_data.explode("misspell").reset_index(drop=True)
misspell_data.drop_duplicates("misspell",inplace=True)
miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))

#Sample of the dict
{v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(20)]}

def misspelled_correction(val):
    for x in val.split(): 
        if x in miss_corr.keys(): 
            val = val.replace(x, miss_corr[x]) 
    return val

data["clean_content"] = data.content.apply(lambda x : misspelled_correction(x))

contractions = pd.read_csv("contractions.csv")
cont_dic = dict(zip(contractions.Contraction, contractions.Meaning))

def cont_to_meaning(val): 
  
    for x in val.split(): 
        if x in cont_dic.keys(): 
            val = val.replace(x, cont_dic[x]) 
    return val

data.clean_content = data.clean_content.apply(lambda x : cont_to_meaning(x))

p.set_options(p.OPT.MENTION, p.OPT.URL)
p.clean("hello guys @alx #sportðŸ”¥ 1245 https://github.com/s/preprocessor")

data["clean_content"]=data.content.apply(lambda x : p.clean(x))

def punctuation(val): 
  
    punctuations = '''()-[]{};:'"\,<>./@#$%^&_~'''
  
    for x in val.lower(): 
        if x in punctuations: 
            val = val.replace(x, " ")
    return val

punctuation("test @ #ldfldlf??? !! ")

data.clean_content = data.clean_content.apply(lambda x : ' '.join(punctuation(emoji.demojize(x)).split()))

def clean_text(val):
    val = misspelled_correction(val)
    val = cont_to_meaning(val)
    val = p.clean(val)
    val = ' '.join(punctuation(emoji.demojize(val)).split())
    
    return val

data = data[data.clean_content != ""]
data.sentiment.value_counts()

sent_to_id  = {"empty":0, "sadness":1,"enthusiasm":2,"neutral":3,"worry":4,
                        "surprise":5,"love":6,"fun":7,"hate":8,"happiness":9,"boredom":10,"relief":11,"anger":12}

data["sentiment_id"] = data['sentiment'].map(sent_to_id)

X_train, X_test, y_train, y_test = train_test_split(data.clean_content,Y, random_state=1995, test_size=0.2, shuffle=True)

max_len = 160
Epoch = 5

def plot_result(df):
    #colors=['#D50000','#000000','#008EF8','#F5B27B','#EDECEC','#D84A09','#019BBD','#FFD000','#7800A0','#098F45','#807C7C','#85DDE9','#F55E10']
    #fig = go.Figure(data=[go.Pie(labels=df.sentiment,values=df.percentage, hole=.3,textinfo='percent',hoverinfo='percent+label',marker=dict(colors=colors, line=dict(color='#000000', width=2)))])
    #fig.show()
    colors={'love':'rgb(213,0,0)','empty':'rgb(0,0,0)',
                    'sadness':'rgb(0,142,248)','enthusiasm':'rgb(245,178,123)',
                    'neutral':'rgb(237,236,236)','worry':'rgb(216,74,9)',
                    'surprise':'rgb(1,155,189)','fun':'rgb(255,208,0)',
                    'hate':'rgb(120,0,160)','happiness':'rgb(9,143,69)',
                    'boredom':'rgb(128,124,124)','relief':'rgb(133,221,233)',
                    'anger':'rgb(245,94,16)'}
    col_2={}
    for i in result.sentiment.to_list():
        col_2[i]=colors[i]
    fig = px.pie(df, values='percentage', names='sentiment',color='sentiment',color_discrete_map=col_2,hole=0.3)
    fig.show()

batch_size = 32

def regular_encode(texts, tokenizer, maxlen=512):
    enc_di = tokenizer.batch_encode_plus(
        texts, 
        return_attention_mask=False, 
        return_token_type_ids=False,
        padding='max_length',
        max_length=maxlen,
        truncation=True
    )
    
    return np.array(enc_di['input_ids'])

def build_model(transformer, max_len=160):
    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    sequence_output = transformer.albert([input_word_ids])[0]
    cls_token = sequence_output[:, 0, :]
    out = Dense(13, activation='softmax')(cls_token)
    
    model = Model(inputs=[input_word_ids], outputs=out, name="albert_model")
    model.compile(Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    print(type(model))
    return model

AUTO = tf.data.experimental.AUTOTUNE
MODEL = 'albert-base-v2'
tokenizer = AutoTokenizer.from_pretrained(MODEL)
X_train_t = regular_encode(X_train.to_list(), tokenizer, maxlen=max_len)
X_test_t = regular_encode(X_test.to_list(), tokenizer, maxlen=max_len)

train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((X_train_t, y_train))
    .repeat()
    .shuffle(1995)
    .batch(32)
    .prefetch(AUTO)
)

valid_dataset = (
    tf.data.Dataset
    .from_tensor_slices((X_test_t, y_test))
    .batch(32)
    .cache()
    .prefetch(AUTO)
)

transformer_layer = TFAutoModel.from_pretrained(MODEL)
albert = build_model(transformer_layer, max_len=max_len)
albert.summary()

n_steps = X_train.shape[0] // batch_size
albert.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=Epoch)

def get_sentiment2(model,text):
    text = clean_text(text)
    #tokenize
    x_test1 = regular_encode([text], tokenizer, maxlen=max_len)
    test1 = (tf.data.Dataset.from_tensor_slices(x_test1).batch(1))
    #test1
    sentiment = model.predict(test1,verbose = 0)
    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]
    result = pd.DataFrame([sent_to_id.keys(),sent]).T
    result.columns = ["sentiment","percentage"]
    result=result[result.percentage !=0]
    return result

result = get_sentiment2(albert,"I love the services Map Communications offered. Their valuable advices helped me grow my sales. Would definitely recommended it to new startups.")
plot_result(result)
result = get_sentiment2(albert,"I ordered a fragile glass statue but it was broken when I received it. I tried contacting customer service but they took a really long time to respond and replace. I will never order from here.")
plot_result(result)
result =get_sentiment2(albert,"The product has multiple features that were suitable for users with different levels of experience.")
plot_result(result)

"""#Save Model and Tokenizer"""

tf.keras.models.save_model(albert, "/content/albert_model", save_format="tf")

tokenizer.save_pretrained("tokenizer")

# Load model and tokenizer from file

tokenizer = transformers.AlbertTokenizerFast.from_pretrained("/content/drive/MyDrive/ALBERT Model/tokenizer")
albert = tf.keras.models.load_model("/content/drive/MyDrive/ALBERT Model/albert_model")

# """#Code to use in app"""

# !pip install tweet-preprocessor
# !pip install emoji
# !pip install transformers
# !pip install tokenizers

# import preprocessor as p
# import numpy as np
# import pandas as pd
# import emoji
# import plotly.express as px
# import tensorflow as tf
# from transformers import AlbertTokenizerFast, TFAutoModel

# sent_to_id  = {"empty":0, "sadness":1,"enthusiasm":2,"neutral":3,"worry":4,
#                "surprise":5,"love":6,"fun":7,"hate":8,"happiness":9,"boredom":10,"relief":11,"anger":12}

# # path_to_spellFile should have path to aspell.txt
# path_to_spellFile = "aspell.txt"
# # path_to_ContractionsFile should have path to contractions.csv
# path_to_contractions = "contractions.csv"

# def setupMispeller():
#   misspell_data = pd.read_csv(path_to_spellFile,sep=":",names=["correction","misspell"])
#   misspell_data.misspell = misspell_data.misspell.str.strip()
#   misspell_data.misspell = misspell_data.misspell.str.split(" ")
#   misspell_data = misspell_data.explode("misspell").reset_index(drop=True)
#   misspell_data.drop_duplicates("misspell",inplace=True)
#   miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))

#   return miss_corr

# def setupContractions():
#   contractions = pd.read_csv(path_to_contractions)
#   cont_dic = dict(zip(contractions.Contraction, contractions.Meaning))

#   return cont_dic

# def misspelled_correction(val, miss_corr):
#   for x in val.split(): 
#     if x in miss_corr.keys(): 
#       val = val.replace(x, miss_corr[x]) 
#   return val

# def cont_to_meaning(val, cont_dic): 
#   for x in val.split(): 
#     if x in cont_dic.keys(): 
#       val = val.replace(x, cont_dic[x]) 
#   return val

# def punctuation(val): 
#   punctuations = '''()-[]{};:'"\,<>./@#$%^&_~'''
#   for x in val.lower(): 
#     if x in punctuations: 
#       val = val.replace(x, " ") 
#   return val

# def clean_text(val, miss_corr, cont_dic):
#   val = misspelled_correction(val, miss_corr)
#   val = cont_to_meaning(val, cont_dic)
#   val = p.clean(val)
#   val = ' '.join(punctuation(emoji.demojize(val)).split())
  
#   return val

# def regular_encode(texts, tokenizer, maxlen=512):
#   enc_di = tokenizer.batch_encode_plus(
#       texts, 
#       return_attention_mask=False, 
#       return_token_type_ids=False,
#       padding='max_length',
#       max_length=maxlen,
#       truncation=True
#   )
    
#   return np.array(enc_di['input_ids'])

# def getSentiment(model, tokenizer, text):
#   #tokenize input text
#   x_test1 = regular_encode([text], tokenizer, maxlen=160)
#   test1 = (tf.data.Dataset.from_tensor_slices(x_test1).batch(1))
#   #test1
#   sentiment = model.predict(test1,verbose = 0)
#   sent = np.round(np.dot(sentiment,100).tolist(),0)[0]
#   result = pd.DataFrame([sent_to_id.keys(),sent]).T
#   result.columns = ["sentiment","percentage"]
#   result=result[result.percentage !=0]
#   return result

# def plot_result(df):
#   colors={'love':'rgb(213,0,0)','empty':'rgb(0,0,0)',
#           'sadness':'rgb(0,142,248)','enthusiasm':'rgb(245,178,123)',
#           'neutral':'rgb(237,236,236)','worry':'rgb(216,74,9)',
#           'surprise':'rgb(1,155,189)','fun':'rgb(255,208,0)',
#           'hate':'rgb(120,0,160)','happiness':'rgb(9,143,69)',
#           'boredom':'rgb(128,124,124)','relief':'rgb(133,221,233)',
#           'anger':'rgb(245,94,16)'}
#   col_2={}
#   for i in result.sentiment.to_list():
#     col_2[i]=colors[i]
#   fig = px.pie(df, values='percentage', names='sentiment',color='sentiment',color_discrete_map=col_2,hole=0.3)
#   return fig

# if __name__ == "__main__":
#   # Sentence input
#   inp = "Today is a good day"

#   miss_corr = setupMispeller()
#   cont_dic = setupContractions()
#   p.set_options(p.OPT.URL, p.OPT.EMOJI)
  
#   val = clean_text(inp, miss_corr, cont_dic)

#   albert = tf.keras.models.load_model("/content/drive/MyDrive/ALBERT Model/albert_model")
#   tokenizer = AlbertTokenizerFast.from_pretrained("/content/drive/MyDrive/ALBERT Model/tokenizer")

#   result = getSentiment(albert, tokenizer, val)
#   plot_result(result)
